# 《知乎页面的图片抓取1》
## 实现目标
在知乎上随便找一个页面，最好是图片较多的，然后将这个页面的图片下载并保存。
## 任务分析
分两个步骤
1. 访问页面，得到所有图片的url  
2. 根据```步骤1```的url,然后下载图片....

  
## 结果示例
暂无

        
## 使用到的nodejs~~库~~包
* [superagent](https://github.com/visionmedia/superagent)
* [async](https://github.com/caolan/async)
* [urlencode](https://github.com/node-modules/urlencode)
* [cheerio](https://github.com/cheeriojs/cheerio)


## 运行方式
**注意，运行环境为windows,mac和linux可能存在兼容性问题**

1. ```git clone https://github.com/isghost/plum```
2. ```cd .\plum\demo7```
3. ```npm start```
